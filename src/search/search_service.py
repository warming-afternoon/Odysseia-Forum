import logging
import re
from typing import Sequence
import rjieba
from sqlmodel import select, func, and_, case, cast, Float
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import selectinload, joinedload

from shared.database import thread_fts_table
from shared.models.thread import Thread
from shared.models.tag import Tag
from shared.models.author import Author
from shared.models.user_collection import UserCollection
from shared.models.thread_tag_link import ThreadTagLink
from search.qo.thread_search import ThreadSearchQuery
from core.tag_service import TagService
from shared.range_parser import parse_range_string
from shared.enum.default_preferences import DefaultPreferences
from shared.time_parser import parse_time_string


class SearchService:
    """å°è£…ä¸æœç´¢ç›¸å…³çš„æ•°æ®åº“æ“ä½œã€‚"""

    def __init__(self, session: AsyncSession, tag_service: TagService):
        self.session = session
        self.tag_service = tag_service

    def _apply_range_filter(self, filters, column, range_str):
        """è§£æèŒƒå›´å­—ç¬¦ä¸²å¹¶åº”ç”¨ä¸ºSQLAlchemyè¿‡æ»¤å™¨"""
        min_val, max_val, min_op, max_op = parse_range_string(range_str)
        if min_val is not None and min_op is not None:
            if min_op == ">=":
                filters.append(column >= min_val)
            else:
                filters.append(column > min_val)

        if max_val is not None and max_op is not None:
            if max_op == "<=":
                filters.append(column <= max_val)
            else:
                filters.append(column < max_val)

    def _apply_ucb1_ranking(
        self,
        statement,
        total_display_count: int,
        exploration_factor: float,
        strength_weight: float,
    ):
        """
        åº”ç”¨ UCB1 ç®—æ³•å¯¹å¸–å­è¿›è¡Œæ’åºã€‚
        Score = W * (x / n) + C * sqrt(ln(N) / n)
        """
        W = strength_weight
        C = exploration_factor
        N = float(max(1, total_display_count))

        # reaction_count as x
        # display_count as n
        x = cast(Thread.reaction_count, Float)
        n = case(
            (Thread.display_count > 0, cast(Thread.display_count, Float)),
            else_=1.0,  # é¿å…é™¤é›¶ï¼Œå¹¶ç»™æ–°å¸–å­æœ€å¤§æ¢ç´¢åŠ æˆ
        )

        exploitation_term = W * (x / n)
        # N/n å¯èƒ½ä¼šéå¸¸å¤§ï¼Œå–å¯¹æ•°é¿å…æº¢å‡º
        exploration_term = C * func.sqrt(func.log(N) / n)

        final_score = (exploitation_term + exploration_term).label("final_score")

        return statement, final_score

    async def search_threads_with_count(
        self,
        query: ThreadSearchQuery,
        *,
        limit: int,
        total_display_count: int,
        exploration_factor: float,
        strength_weight: float,
        offset: int = 0,
        exclude_thread_ids: Sequence[int | str] | None = None,
    ) -> tuple[Sequence[Thread], int]:
        """
        æ ¹æ®æœç´¢æ¡ä»¶æœç´¢å¸–å­å¹¶åˆ†é¡µ
        """
        try:
            # è§£ææ—¶é—´å­—ç¬¦ä¸²
            try:
                created_after_dt = parse_time_string(query.created_after)
                created_before_dt = parse_time_string(query.created_before)
                active_after_dt = parse_time_string(query.active_after)
                active_before_dt = parse_time_string(query.active_before)
            except ValueError as e:
                # ç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸ºåœ¨ Modal ä¸­å·²ç»éªŒè¯è¿‡
                logging.warning(f"æ—¶é—´å­—ç¬¦ä¸²è§£æå¤±è´¥: {e}")
                raise

            # --- æ­¥éª¤ 0: è§£ææ ‡ç­¾ ---
            resolved_include_tag_ids = [
                id
                for name in query.include_tags
                for id in self.tag_service.get_ids_by_name(name)
            ]
            resolved_exclude_tag_ids = [
                id
                for name in query.exclude_tags
                for id in self.tag_service.get_ids_by_name(name)
            ]

            # --- æ­¥éª¤ 1: æ„å»ºåŸºç¡€è¿‡æ»¤å™¨åˆ—è¡¨ (é™¤äº†åé€‰å…³é”®è¯) ---
            filters = []
            # åªæœç´¢ not_found_count == 0 çš„å¸–å­ï¼Œé¿å…æ˜¾ç¤ºè¢«è½¯åˆ é™¤çš„å¸–å­
            filters.append(Thread.not_found_count == 0)
            if query.channel_ids:
                filters.append(Thread.channel_id.in_(query.channel_ids))  # type: ignore
            if exclude_thread_ids:
                normalized_ids = []
                for tid in exclude_thread_ids:
                    try:
                        normalized_ids.append(int(tid))
                    except (TypeError, ValueError):
                        continue
                if normalized_ids:
                    filters.append(~Thread.thread_id.in_(normalized_ids))  # type: ignore

            final_include_author_ids = (
                set(query.include_authors) if query.include_authors else set()
            )

            if query.author_name:
                normalized_author_name = query.author_name.strip()
                if normalized_author_name:
                    # ä½¿ç”¨å­æŸ¥è¯¢æ¥æŸ¥æ‰¾åŒ¹é…çš„ä½œè€… ID
                    search_pattern = f"%{normalized_author_name}%"
                    author_subquery = select(Author.id).where(
                        (func.lower(Author.name) == normalized_author_name.lower())
                        | (Author.global_name.like(search_pattern))  # type: ignore
                        | (Author.display_name.like(search_pattern))  # type: ignore
                    )  # type: ignore
                    author_result = await self.session.execute(author_subquery)
                    matched_author_ids = set(author_result.scalars().all())

                    if query.include_authors:
                        # å¦‚æœåŒæ—¶æŒ‡å®šäº†IDå’Œåç§°ï¼Œåˆ™å–äº¤é›†
                        final_include_author_ids.intersection_update(matched_author_ids)
                    else:
                        final_include_author_ids = matched_author_ids

            # åº”ç”¨ä½œè€…è¿‡æ»¤å™¨
            if final_include_author_ids:
                filters.append(
                    Thread.author_id.in_(list(final_include_author_ids))  # type: ignore
                )
            if query.exclude_authors:
                filters.append(
                    Thread.author_id.notin_(query.exclude_authors)  # type: ignore
                )

            # --- èŒƒå›´è¿‡æ»¤---
            if query.reaction_count_range != (
                DefaultPreferences.DEFAULT_NUMERIC_RANGE.value
            ):
                self._apply_range_filter(
                    filters, Thread.reaction_count, query.reaction_count_range
                )
            if query.reply_count_range != (
                DefaultPreferences.DEFAULT_NUMERIC_RANGE.value
            ):
                self._apply_range_filter(
                    filters, Thread.reply_count, query.reply_count_range
                )

            if created_after_dt:
                filters.append(Thread.created_at >= created_after_dt)
            if created_before_dt:
                filters.append(Thread.created_at <= created_before_dt)

            # å¯¹å¯èƒ½ä¸º None (è™½ç„¶ä¸å¤ªå¯èƒ½ï¼Œæˆ‘è¯´)çš„ last_active_at è¿›è¡Œå®‰å…¨å¤„ç†
            if active_after_dt or active_before_dt:
                conditions = [Thread.last_active_at != None]  # noqa: E711
                if active_after_dt:
                    conditions.append(Thread.last_active_at >= active_after_dt)  # type: ignore
                if active_before_dt:
                    conditions.append(Thread.last_active_at <= active_before_dt)  # type: ignore
                filters.append(and_(*conditions))

            # -- æ ‡ç­¾è¿‡æ»¤ --
            if resolved_include_tag_ids:
                if query.tag_logic == "and":
                    # TODO : è€ƒè™‘ç²¾ç®€
                    for tag_name in query.include_tags:
                        ids_for_name = self.tag_service.get_ids_by_name(tag_name)
                        if ids_for_name:
                            filters.append(Thread.tags.any(Tag.id.in_(ids_for_name)))  # type: ignore
                else:
                    filters.append(
                        Thread.tags.any(Tag.id.in_(resolved_include_tag_ids))  # type: ignore
                    )  # type: ignore
            if resolved_exclude_tag_ids:
                filters.append(
                    ~Thread.tags.any(Tag.id.in_(resolved_exclude_tag_ids))  # type: ignore
                )

            # --- æ­¥éª¤ 2: å•ç‹¬å¤„ç†åé€‰å…³é”®è¯ ---
            if query.exclude_keywords:
                exemption_markers = (
                    query.exclude_keyword_exemption_markers
                    if query.exclude_keyword_exemption_markers is not None
                    else ["ç¦", "ğŸˆ²"]
                )
                exclude_keywords_list = [
                    kw.strip()
                    for kw in re.split(r"[,ï¼Œ/\s]+", query.exclude_keywords)
                    if kw.strip()
                ]

                all_exclude_parts = []
                for keyword in exclude_keywords_list:
                    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†è¯ï¼Œå¹¶å¤„ç†å‰ç¼€
                    tokens = [tok.strip() for tok in rjieba.cut(keyword) if tok.strip()]
                    if not tokens:
                        continue

                    # æ„å»ºåŒ¹é…éƒ¨åˆ†
                    match_parts = [f'"{tok}"' for tok in tokens[:-1]]
                    match_parts.append(f'"{tokens[-1]}"*')  # æœ€åä¸€ä¸ªè¯å…ƒåŠ å‰ç¼€
                    match_expr = " AND ".join(match_parts)

                    # åªæœ‰åœ¨è±å…æ ‡è®°åˆ—è¡¨éç©ºæ—¶æ‰æ„å»ºè±å…é€»è¾‘
                    if exemption_markers:
                        # åªç”¨å…³é”®è¯çš„ç¬¬ä¸€ä¸ªåˆ†è¯æ¥æ£€æŸ¥è±å…
                        # ä»¥é¿å… NEAR æ“ä½œç¬¦å’Œå‰ç¼€ï¼ˆ*ï¼‰çš„å…¼å®¹æ€§é—®é¢˜
                        first_token = tokens[0]
                        exemption_clauses = [
                            f'NEAR("{first_token}" "{marker}", 4)'  # ä¹Ÿå¯ä»¥é€‚å½“å‡å°è·ç¦»
                            for marker in exemption_markers
                        ]
                        exemption_match_str = f"({' OR '.join(exemption_clauses)})"

                        # æ„å»ºå¸¦æœ‰ NOT çš„ FTS è¡¨è¾¾å¼
                        all_exclude_parts.append(
                            f"({match_expr}) NOT {exemption_match_str}"
                        )
                    else:
                        # å¦‚æœæ²¡æœ‰è±å…æ ‡è®°ï¼Œç›´æ¥æ’é™¤å…³é”®è¯
                        all_exclude_parts.append(f"({match_expr})")

                if all_exclude_parts:
                    final_exclude_expr = " OR ".join(all_exclude_parts)
                    # åˆ›å»ºä¸€ä¸ªå­æŸ¥è¯¢ï¼Œä¸“é—¨ç”¨äºæ‰¾å‡ºè¦æ’é™¤çš„ thread ID
                    exclude_ids_subquery = select(thread_fts_table.c.rowid).where(
                        thread_fts_table.c.thread_fts.op("MATCH")(final_exclude_expr)
                    )
                    # å°†æ’é™¤é€»è¾‘æ·»åŠ åˆ°ä¸»è¿‡æ»¤å™¨ä¸­
                    filters.append(Thread.id.not_in(exclude_ids_subquery))  # type: ignore

            # --- æ­¥éª¤ 3: ç»„åˆæ­£é€‰å…³é”®è¯å’Œå…¶ä»–è¿‡æ»¤å™¨ ---
            base_stmt = select(Thread.id).distinct()

            if query.user_id_for_collection_search:
                # å¦‚æœæ˜¯æ”¶è—æœç´¢ï¼Œåˆ™å¿…é¡» JOIN user_collection è¡¨ï¼ˆå¸–å­ç±»å‹ï¼‰
                from shared.enum.collection_type import CollectionType

                base_stmt = base_stmt.join(
                    UserCollection,
                    and_(
                        Thread.thread_id == UserCollection.target_id,
                        UserCollection.target_type == CollectionType.THREAD,
                    ),  # type: ignore
                )
                # å¹¶å°†ç”¨æˆ·IDä½œä¸ºé¦–è¦è¿‡æ»¤æ¡ä»¶
                filters.append(
                    UserCollection.user_id == query.user_id_for_collection_search
                )

            needs_fts_join = query.keywords  # åªåœ¨æœ‰æ­£é€‰å…³é”®è¯æ—¶æ‰éœ€è¦JOIN
            if needs_fts_join:
                base_stmt = base_stmt.join(
                    thread_fts_table,
                    Thread.id == thread_fts_table.c.rowid,  # type: ignore
                )

            # -- æ­£é€‰å…³é”®è¯ --
            if query.keywords:
                keywords_str = query.keywords.replace("ï¼Œ", ",").replace("ï¼", "/")
                and_groups = [
                    group.strip() for group in keywords_str.split(",") if group.strip()
                ]
                for group in and_groups:
                    or_keywords = []
                    for kw in group.split("/"):
                        kw = kw.strip()
                        if not kw:
                            continue
                        # æ£€æŸ¥æ˜¯å¦æ˜¯ç²¾ç¡®åŒ¹é…ï¼ˆç”¨å¼•å·åŒ…å›´ï¼‰
                        if kw.startswith('"') and kw.endswith('"') and len(kw) > 2:
                            # ç²¾ç¡®åŒ¹é…ï¼šç§»é™¤å¼•å·ï¼Œä¸æ·»åŠ *å‰ç¼€
                            exact_kw = kw[1:-1].strip()
                            if exact_kw:
                                or_keywords.append(f'"{exact_kw}"')
                        else:
                            # æ™®é€šå…³é”®è¯ï¼šæ·»åŠ *å‰ç¼€åŒ¹é…
                            or_keywords.append(f"{kw}*")

                    if or_keywords:
                        filters.append(
                            thread_fts_table.c.thread_fts.op("MATCH")(
                                " OR ".join(or_keywords)
                            )
                        )

            # åº”ç”¨æ‰€æœ‰è¿‡æ»¤å™¨
            if filters:
                base_stmt = base_stmt.where(and_(*filters))

            # --- æ­¥éª¤ 4: è®¡æ•° ---
            count_stmt = select(func.count()).select_from(base_stmt.alias("sub"))
            count_result = await self.session.execute(count_stmt)
            total_count = count_result.scalar_one_or_none() or 0

            if total_count == 0:
                return [], 0

            # --- æ­¥éª¤ 5: è·å–åˆ†é¡µæ•°æ®å’Œæ’åº ---
            final_select_stmt = (
                select(Thread)
                .where(Thread.id.in_(base_stmt))  # type: ignore
                .options(
                    selectinload(Thread.tags),  # type: ignore
                    joinedload(Thread.author),  # type: ignore
                )
            )

            order_by = None

            # å¦‚æœæ˜¯è‡ªå®šä¹‰æœç´¢ï¼Œåˆ™ä½¿ç”¨å…¶åŸºç¡€æ’åºç®—æ³•ï¼Œå¦åˆ™ä½¿ç”¨ä¸»æ’åºç®—æ³•
            effective_sort_method = (
                query.custom_base_sort
                if query.sort_method == "custom"
                else query.sort_method
            )

            if effective_sort_method == "comprehensive":
                final_select_stmt, final_score_expr = self._apply_ucb1_ranking(
                    final_select_stmt,
                    total_display_count,
                    exploration_factor,
                    strength_weight,
                )
                order_by = (
                    final_score_expr.desc()
                    if query.sort_order == "desc"
                    else final_score_expr.asc()
                )
            else:
                sort_col_name = (
                    effective_sort_method
                    if hasattr(Thread, effective_sort_method)
                    else "last_active_at"
                )
                sort_col = getattr(Thread, sort_col_name)
                order_by = (
                    sort_col.desc() if query.sort_order == "desc" else sort_col.asc()
                )

            if order_by is not None:
                final_select_stmt = final_select_stmt.order_by(order_by)

            if offset:
                final_select_stmt = final_select_stmt.offset(offset)
            final_select_stmt = final_select_stmt.limit(limit)

            result = await self.session.execute(final_select_stmt)
            threads = result.scalars().all()

            return threads, total_count

        except Exception:
            logging.error(
                "Error during search_threads_with_count execution", exc_info=True
            )
            raise

    async def get_tags_for_author(self, author_id: int) -> Sequence[Tag]:
        """è·å–æŒ‡å®šä½œè€…å‘å¸ƒè¿‡çš„æ‰€æœ‰å¸–å­çš„å”¯ä¸€æ ‡ç­¾åˆ—è¡¨"""
        statement = (
            select(Tag)
            .join(Thread, Tag.threads)  # type: ignore
            .where(Thread.author_id == author_id)  # type: ignore
            .distinct()
        )
        result = await self.session.execute(statement)
        return result.scalars().all()

    async def get_tags_for_collections(self, user_id: int) -> Sequence[Tag]:
        """è·å–æŒ‡å®šç”¨æˆ·æ”¶è—çš„æ‰€æœ‰å¸–å­çš„å”¯ä¸€æ ‡ç­¾åˆ—è¡¨"""
        from shared.enum.collection_type import CollectionType

        statement = (
            select(Tag)
            .join(ThreadTagLink, Tag.id == ThreadTagLink.tag_id)  # type: ignore
            .join(Thread, ThreadTagLink.thread_id == Thread.id)  # type: ignore
            .join(
                UserCollection,
                and_(
                    Thread.thread_id == UserCollection.target_id,
                    UserCollection.target_type == CollectionType.THREAD,
                ),  # type: ignore
            )
            .where(UserCollection.user_id == user_id)
            .distinct()
        )
        result = await self.session.execute(statement)
        return result.scalars().all()
